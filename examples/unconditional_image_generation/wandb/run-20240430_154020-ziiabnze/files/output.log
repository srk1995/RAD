04/30/2024 15:40:26 - INFO - __main__ - ***** Running training *****
04/30/2024 15:40:26 - INFO - __main__ -   Num examples = 28000
04/30/2024 15:40:26 - INFO - __main__ -   Num Epochs = 300
04/30/2024 15:40:26 - INFO - __main__ -   Instantaneous batch size per device = 64
04/30/2024 15:40:27 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64
04/30/2024 15:40:27 - INFO - __main__ -   Gradient Accumulation steps = 1
04/30/2024 15:40:27 - INFO - __main__ -   Total optimization steps = 131400
04/30/2024 15:40:27 - INFO - __main__ -   Image path = ddpm-model-mattymchen/celeba-hq-256/Local-celeba-hq-2000-1000-0/FID images
/snap/pycharm-professional/384/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_xml.py:346: FutureWarning: Accessing config attribute `__len__` directly via 'LocalUNet2DModel' object attribute is deprecated. Please access '__len__' over 'LocalUNet2DModel's config object instead, e.g. 'unet.config.__len__'.
  elif hasattr(v, '__len__') and not is_string(v):
/home/srk1995/PycharmProjects/diffusers/src/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `__len__` directly via 'InPaintLocalDDPMPipeline' object attribute is deprecated. Please access '__len__' over 'InPaintLocalDDPMPipeline's config object instead, e.g. 'scheduler.config.__len__'.
  deprecate("direct config name access", "1.0.0", deprecation_message, standard_warn=False)
Checkpoint 'latest' does not exist. Starting a new training run.
Traceback (most recent call last):
  File "/home/srk1995/anaconda3/envs/ADM/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/srk1995/PycharmProjects/diffusers/src/diffusers/pipelines/localddpm/inpainting_pipeline_localddpm.py", line 109, in __call__
    self.scheduler.set_variables(mask, len(self.scheduler.m_steps))
  File "/home/srk1995/PycharmProjects/diffusers/src/diffusers/schedulers/scheduling_localddpm.py", line 284, in set_variables
    t = torch.where(self.random_steps[None, :].to(t.device) == t[:, None])[1]
