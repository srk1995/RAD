04/30/2024 15:41:24 - INFO - __main__ - ***** Running training *****
04/30/2024 15:41:25 - INFO - __main__ -   Num examples = 28000
04/30/2024 15:41:25 - INFO - __main__ -   Num Epochs = 300
04/30/2024 15:41:25 - INFO - __main__ -   Instantaneous batch size per device = 64
04/30/2024 15:41:25 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64
04/30/2024 15:41:25 - INFO - __main__ -   Gradient Accumulation steps = 1
04/30/2024 15:41:26 - INFO - __main__ -   Total optimization steps = 131400
04/30/2024 15:41:26 - INFO - __main__ -   Image path = ddpm-model-mattymchen/celeba-hq-256/Local-celeba-hq-2000-1000-0/FID images
Traceback (most recent call last):
  File "/home/srk1995/anaconda3/envs/ADM/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/srk1995/PycharmProjects/diffusers/src/diffusers/pipelines/localddpm/inpainting_pipeline_localddpm.py", line 109, in __call__
    self.scheduler.set_variables(mask, torch.tensor([len(self.scheduler.m_steps)]).to(self.device))
  File "/home/srk1995/PycharmProjects/diffusers/src/diffusers/schedulers/scheduling_localddpm.py", line 287, in set_variables
    self.a_t_prepare[mask_idx] = (1 - self.nu * mask[mask_idx])
RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)
