06/04/2024 16:48:27 - INFO - __main__ - ***** Running training *****
06/04/2024 16:48:27 - INFO - __main__ -   Num examples = 28000
06/04/2024 16:48:28 - INFO - __main__ -   Num Epochs = 2000
06/04/2024 16:48:28 - INFO - __main__ -   Instantaneous batch size per device = 16
06/04/2024 16:48:28 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
06/04/2024 16:48:28 - INFO - __main__ -   Gradient Accumulation steps = 1
06/04/2024 16:48:28 - INFO - __main__ -   Total optimization steps = 3500000
06/04/2024 16:48:28 - INFO - __main__ -   Image path = ddpm-model-mattymchen/celeba-hq-64/crop_Local-celeba-hq-2000-1000-0-b_bar/FID images_430000
06/04/2024 16:48:29 - INFO - accelerate.accelerator - Loading states from ddpm-model-mattymchen/celeba-hq-64/crop_Local-celeba-hq-2000-1000-0-b_bar/checkpoint-430000
Resuming from checkpoint checkpoint-430000
06/04/2024 16:48:29 - INFO - accelerate.checkpointing - All model weights loaded successfully
06/04/2024 16:48:30 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
06/04/2024 16:48:30 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
06/04/2024 16:48:31 - INFO - accelerate.checkpointing - All dataloader sampler states loaded successfully
06/04/2024 16:48:31 - INFO - accelerate.checkpointing - All random states loaded successfully
06/04/2024 16:48:31 - INFO - accelerate.accelerator - Loading in 0 custom states
/home/srk1995/anaconda3/envs/ADM/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
  0%|          | 0/250 [00:00<?, ?it/s]/home/srk1995/PycharmProjects/diffusers/src/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `__len__` directly via 'LocalDDPMPipeline' object attribute is deprecated. Please access '__len__' over 'LocalDDPMPipeline's config object instead, e.g. 'scheduler.config.__len__'.
  deprecate("direct config name access", "1.0.0", deprecation_message, standard_warn=False)












100%|██████████| 250/250 [00:30<00:00,  8.19it/s]
/home/srk1995/PycharmProjects/diffusers/src/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `__len__` directly via 'LocalDDPMPipeline' object attribute is deprecated. Please access '__len__' over 'LocalDDPMPipeline's config object instead, e.g. 'scheduler.config.__len__'.
  deprecate("direct config name access", "1.0.0", deprecation_message, standard_warn=False)
Traceback (most recent call last):
  File "/home/srk1995/PycharmProjects/diffusers/examples/unconditional_image_generation/image_sample.py", line 672, in <module>
    main(args)
  File "/home/srk1995/PycharmProjects/diffusers/examples/unconditional_image_generation/image_sample.py", line 646, in main
    images = pipeline(
  File "/home/srk1995/anaconda3/envs/ADM/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/srk1995/PycharmProjects/diffusers/src/diffusers/pipelines/localddpm/pipeline_localddpm.py", line 150, in __call__
    if output_type == "pil":
